from flask import Blueprint, request, jsonify, render_template
import jwt
from app import app, db
from app.models import Input, Prediction
import pickle
import pandas as pd
import numpy as np
import xgboost as xgb
import os
import json
import random
import string

predict_bp = Blueprint('predict', __name__)

# ëª¨ë¸ ë¡œë“œ
try:
    BASE_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))  # `app/`ì˜ ìƒìœ„ í´ë”ë¡œ ì´ë™
    MODEL_PATH = os.path.join(BASE_DIR, "model", "final_model.json")  # ëª¨ë¸ ê²½ë¡œ ì„¤ì •

    # ğŸ”¹ ëª¨ë¸ ë¡œë“œ
    model = xgb.XGBClassifier()
    model.load_model(MODEL_PATH) 
except FileNotFoundError:
    raise RuntimeError("âŒ ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

def get_user_id_from_token():
    """ JWT í† í°ì—ì„œ user_id ì¶”ì¶œ """
    token = request.headers.get("Authorization")
    if not token:
        return None, None  # ë¹„íšŒì›ì´ë©´ None ë°˜í™˜
    try:
        decoded = jwt.decode(token, app.config["SECRET_KEY"], algorithms=["HS256"])
        return decoded["user_id"], None  # ë¡œê·¸ì¸í•œ íšŒì›ì´ë©´ user_id ë°˜í™˜
    except jwt.ExpiredSignatureError:
        return None, jsonify({"message": "Token expired"})  # í† í° ë§Œë£Œ
    except jwt.InvalidTokenError:
        return None, jsonify({"message": "Invalid token"})  # ì˜ëª»ëœ í† í°

def get_season(month):
    """ ë°ì´í„° ì „ì²˜ë¦¬ ì¤‘ ì¼ë¶€ """
    if month in [3, 4, 5]:
        return 'ë´„'
    elif month in [6, 7, 8]:
        return 'ì—¬ë¦„'
    elif month in [9, 10, 11]:
        return 'ê°€ì„'
    else:
        return 'ê²¨ìš¸'
    
def preprocess_for_file(df):
    try:
        sample = df.copy()
        sample2 = df.copy()

        if sample.empty:
            raise ValueError("âŒ ì…ë ¥ ë°ì´í„°ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")

        # ê²°ì¸¡ ì—¬ë¶€ë¥¼ ì´ì§„ ë³€ìˆ˜ë¡œ ë³€í™˜
        for col in sample2.columns:
            sample2[f'{col}'] = sample2[col].isna().astype(int)
        sample2['ê²°ì¸¡ì¹˜ê°œìˆ˜'] = sample2.sum(axis=1)
        sample['ê²°ì¸¡ì¹˜ê°œìˆ˜'] = sample2['ê²°ì¸¡ì¹˜ê°œìˆ˜']
        
        train_medians = pd.read_csv('./saved/train_medians.csv')
        train_medians_dict = train_medians.set_index("column_name")["median_value"].to_dict()  # train_mediansê°€ DataFrameì´ë¼ë©´ ë³€í™˜
        
        # ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ë§Œ ì¶”ì¶œ
        numeric_cols = [col for col in sample.select_dtypes(include=['number']).columns]
        sample[numeric_cols] = sample[numeric_cols].fillna(sample[numeric_cols].apply(lambda col: train_medians_dict.get(col.name, col.median())))
        
        # ìŠ¤ì¼€ì¼ëŸ¬ ë° KNN ëª¨ë¸ ë¡œë“œ
        scaler = pickle.load(open("./saved/scaler.pkl", "rb"))
        knn_hc = pickle.load(open("./saved/knn_hc.pkl", "rb"))
        knn_dbscan = pickle.load(open("./saved/knn_dbscan.pkl", "rb"))
        X_test = sample[['ì „ìš©ë©´ì ', 'ë°©ìˆ˜', 'ìš•ì‹¤ìˆ˜']]
        X_test_scaled = scaler.transform(X_test)

        # KNN ì˜ˆì¸¡ ìˆ˜í–‰
        sample["ë§¤ë¬¼_HC"] = knn_hc.predict(X_test_scaled)
        sample["ë§¤ë¬¼_DBSCAN"] = knn_dbscan.predict(X_test_scaled)
        
        # ê¸ˆì•¡ ë‹¨ìœ„ ë³€í™˜
        # sample["ë³´ì¦ê¸ˆ"] = sample["ë³´ì¦ê¸ˆ"] / 10000
        # sample["ì›”ì„¸"] = sample["ì›”ì„¸"] / 10000
        
        sample['ì›”ì„¸+ê´€ë¦¬ë¹„'] = sample['ì›”ì„¸'] + sample['ê´€ë¦¬ë¹„']
        sample['ë³´ì¦ê¸ˆ_ì›”ì„¸ê´€ë¦¬ë¹„_ë¹„ìœ¨'] = sample['ì›”ì„¸+ê´€ë¦¬ë¹„'] / sample['ë³´ì¦ê¸ˆ']
        sample['ì „ìš©ë©´ì _ê°€ê²©_ë¹„ìœ¨'] = sample['ë³´ì¦ê¸ˆ_ì›”ì„¸ê´€ë¦¬ë¹„_ë¹„ìœ¨'] / sample['ì „ìš©ë©´ì ']
        
        scaler2 = pickle.load(open("./saved/scaler2.pkl", "rb"))
        knn_kmedoids = pickle.load(open("./saved/knn_kmedoids.pkl", "rb"))
        
        X_test2 = sample[['ë§¤ë¬¼_HC', 'ë§¤ë¬¼_DBSCAN', 'ì „ìš©ë©´ì _ê°€ê²©_ë¹„ìœ¨', 'ë³´ì¦ê¸ˆ_ì›”ì„¸ê´€ë¦¬ë¹„_ë¹„ìœ¨']]
        X_test_scaled2 = scaler2.transform(X_test2)
        
        sample["ì§€ì—­_KMedoids"] = knn_kmedoids.predict(X_test_scaled2)
    
        sample['ê²Œì¬ì¼'] = pd.to_datetime(sample['ê²Œì¬ì¼'], errors='coerce')
        sample['ê³„ì ˆ'] = sample['ê²Œì¬ì¼'].dt.month.apply(get_season)
        
        date_max = pickle.load(open("./saved/date_max.pkl", "rb"))
        sample['ë§¤ë¬¼_ë“±ë¡_ê²½ê³¼ì¼'] = (date_max - sample['ê²Œì¬ì¼']).dt.days
        
        sample = pd.get_dummies(sample, columns=['ë§¤ë¬¼í™•ì¸ë°©ì‹', 'ë°©í–¥', 'ì£¼ì°¨ê°€ëŠ¥ì—¬ë¶€', 'ê³„ì ˆ'], drop_first=True)
        one_hot_columns = [col for col in sample.columns if 'ë§¤ë¬¼í™•ì¸ë°©ì‹' in col or 'ë°©í–¥' in col or 'ì£¼ì°¨ê°€ëŠ¥ì—¬ë¶€' in col or 'ê³„ì ˆ' in col]
        sample[one_hot_columns] = sample[one_hot_columns].astype(int)
        
        sample = sample.drop(columns = ['ID', 'ì¤‘ê°œì‚¬ë¬´ì†Œ', 'ì œê³µí”Œë«í¼', 'ê²Œì¬ì¼', 'ë§¤ë¬¼_DBSCAN', 'ì›”ì„¸+ê´€ë¦¬ë¹„', 'ë³´ì¦ê¸ˆ_ì›”ì„¸ê´€ë¦¬ë¹„_ë¹„ìœ¨'], axis = 1)
        sample = pd.get_dummies(sample, columns=['ë§¤ë¬¼_HC', 'ì§€ì—­_KMedoids'], drop_first=True)
        one_hot_columns = [col for col in sample.columns if 'HC' in col or 'KMedoids' in col]
        sample[one_hot_columns] = sample[one_hot_columns].astype(int)
        return sample
    except Exception as e:
        raise ValueError(f"ğŸš¨ ë°ì´í„° ì „ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
    
def preprocess_for_one(df):
    try:
        sample = df.copy()
        sample2 = df.copy()

        if sample.empty:
            raise ValueError("âŒ ì…ë ¥ ë°ì´í„°ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")

        # ê²°ì¸¡ ì—¬ë¶€ë¥¼ ì´ì§„ ë³€ìˆ˜ë¡œ ë³€í™˜
        for col in sample2.columns:
            sample2[f'{col}'] = sample2[col].isna().astype(int)
        sample2['ê²°ì¸¡ì¹˜ê°œìˆ˜'] = sample2.sum(axis=1)
        sample['ê²°ì¸¡ì¹˜ê°œìˆ˜'] = sample2['ê²°ì¸¡ì¹˜ê°œìˆ˜']
        
        train_medians = pd.read_csv('./saved/train_medians.csv')
        train_medians_dict = train_medians.set_index("column_name")["median_value"].to_dict()  # train_mediansê°€ DataFrameì´ë¼ë©´ ë³€í™˜
        
        # ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ë§Œ ì¶”ì¶œ
        numeric_cols = [col for col in sample.select_dtypes(include=['number']).columns]
        sample[numeric_cols] = sample[numeric_cols].fillna(sample[numeric_cols].apply(lambda col: train_medians_dict.get(col.name, col.median())))
        
        # ìŠ¤ì¼€ì¼ëŸ¬ ë° KNN ëª¨ë¸ ë¡œë“œ
        scaler = pickle.load(open("./saved/scaler.pkl", "rb"))
        knn_hc = pickle.load(open("./saved/knn_hc.pkl", "rb"))
        knn_dbscan = pickle.load(open("./saved/knn_dbscan.pkl", "rb"))
        X_test = sample[['ì „ìš©ë©´ì ', 'ë°©ìˆ˜', 'ìš•ì‹¤ìˆ˜']]
        X_test_scaled = scaler.transform(X_test)

        X_test_scaled = scaler.transform(X_test)  # ìŠ¤ì¼€ì¼ë§ ìˆ˜í–‰

        # KNN ì˜ˆì¸¡ ìˆ˜í–‰
        sample["ë§¤ë¬¼_HC"] = knn_hc.predict(X_test_scaled)
        sample["ë§¤ë¬¼_DBSCAN"] = knn_dbscan.predict(X_test_scaled)

        # ê¸ˆì•¡ ë‹¨ìœ„ ë³€í™˜
        # sample["ë³´ì¦ê¸ˆ"] = sample["ë³´ì¦ê¸ˆ"] / 10000
        # sample["ì›”ì„¸"] = sample["ì›”ì„¸"] / 10000
        
        sample['ì›”ì„¸+ê´€ë¦¬ë¹„'] = sample['ì›”ì„¸'] + sample['ê´€ë¦¬ë¹„']
        sample['ë³´ì¦ê¸ˆ_ì›”ì„¸ê´€ë¦¬ë¹„_ë¹„ìœ¨'] = sample['ì›”ì„¸+ê´€ë¦¬ë¹„'] / sample['ë³´ì¦ê¸ˆ']
        sample['ì „ìš©ë©´ì _ê°€ê²©_ë¹„ìœ¨'] = sample['ë³´ì¦ê¸ˆ_ì›”ì„¸ê´€ë¦¬ë¹„_ë¹„ìœ¨'] / sample['ì „ìš©ë©´ì ']
        
        scaler2 = pickle.load(open("./saved/scaler2.pkl", "rb"))
        knn_kmedoids = pickle.load(open("./saved/knn_kmedoids.pkl", "rb"))
        
        X_test2 = sample[['ë§¤ë¬¼_HC', 'ë§¤ë¬¼_DBSCAN', 'ì „ìš©ë©´ì _ê°€ê²©_ë¹„ìœ¨', 'ë³´ì¦ê¸ˆ_ì›”ì„¸ê´€ë¦¬ë¹„_ë¹„ìœ¨']]
        X_test_scaled2 = scaler2.transform(X_test2)
        
        sample["ì§€ì—­_KMedoids"] = knn_kmedoids.predict(X_test_scaled2)
        sample['ê²Œì¬ì¼'] = pd.to_datetime(sample['ê²Œì¬ì¼'], errors='coerce')
        sample['ê³„ì ˆ'] = sample['ê²Œì¬ì¼'].dt.month.apply(get_season)
        
        date_max = pickle.load(open("./saved/date_max.pkl", "rb"))
        sample['ë§¤ë¬¼_ë“±ë¡_ê²½ê³¼ì¼'] = (date_max - sample['ê²Œì¬ì¼']).dt.days
        
        # ì›-í•« ì¸ì½”ë”©ì„ ì ìš©í•  ì»¬ëŸ¼ ë° ì œì™¸í•  ê°’
        one_hot_columns = {
            "ë§¤ë¬¼í™•ì¸ë°©ì‹": ["í˜„ì¥í™•ì¸", "ì „í™”í™•ì¸"],  # "ì„œë¥˜í™•ì¸"ì€ ìƒì„±í•˜ì§€ ì•ŠìŒ
            "ë°©í–¥": ["ì„œí–¥", "ë™í–¥", "ë‚¨í–¥", "ë¶ë™í–¥", "ë¶í–¥", "ë‚¨ì„œí–¥", "ë¶ì„œí–¥"],  # "ë‚¨ë™í–¥"ì€ ìƒì„±í•˜ì§€ ì•ŠìŒ
            "ì£¼ì°¨ê°€ëŠ¥ì—¬ë¶€": ["ë¶ˆê°€ëŠ¥"],  # "ê°€ëŠ¥"ì€ ìƒì„±í•˜ì§€ ì•ŠìŒ
            "ê³„ì ˆ": ["ë´„", "ì—¬ë¦„", "ê²¨ìš¸"]  # "ê°€ì„"ì€ ìƒì„±í•˜ì§€ ì•ŠìŒ
        }

        # ì›-í•« ì¸ì½”ë”© ì ìš© (drop_first=True íš¨ê³¼ ì ìš©)
        for col, categories in one_hot_columns.items():
            for cat in categories:
                sample[f"{col}_{cat}"] = (sample[col] == cat).astype(int)

        # ê¸°ì¡´ ì¹´í…Œê³ ë¦¬ ì»¬ëŸ¼ ì‚­ì œ
        sample = sample.drop(columns=one_hot_columns.keys(), errors="ignore")
        sample = sample.drop(columns = ['ID', 'ì¤‘ê°œì‚¬ë¬´ì†Œ', 'ì œê³µí”Œë«í¼', 'ê²Œì¬ì¼', 'ë§¤ë¬¼_DBSCAN', 'ì›”ì„¸+ê´€ë¦¬ë¹„', 'ë³´ì¦ê¸ˆ_ì›”ì„¸ê´€ë¦¬ë¹„_ë¹„ìœ¨'], axis = 1)
        
        for i in range(2, 7): 
            sample[f"ë§¤ë¬¼_HC_{i}"] = 0
        
        for i in range(1, 11):
            sample[f"ì§€ì—­_KMedoids_{i}"] = 0
        
        hc_value = sample['ë§¤ë¬¼_HC'].iloc[0]  # ì²« ë²ˆì§¸ rowì˜ ê°’
        if hc_value != 1:  # ì›í•«ì¸ì½”ë”©ìœ¼ë¡œ ì¸í•´ ë§¤ë¬¼_HC_1 ì»¬ëŸ¼ ìƒì„± x
            sample[f"ë§¤ë¬¼_HC_{hc_value}"] = 1
        
        km_value = sample['ì§€ì—­_KMedoids'].iloc[0]  # ì²« ë²ˆì§¸ rowì˜ ê°’
        if km_value != 0:  # ì›í•«ì¸ì½”ë”©ìœ¼ë¡œ ì¸í•´ ì§€ì—­_KMedoids_0 ì»¬ëŸ¼ ìƒì„± x
            sample[f"ì§€ì—­_KMedoids_{km_value}"] = 1
            
        sample = sample.drop(columns = ['ë§¤ë¬¼_HC', 'ì§€ì—­_KMedoids'], axis = 1)
        print("ìµœì¢… ì»¬ëŸ¼ ìˆ˜ : ", len(sample.columns))
        print("ìµœì¢… ì»¬ëŸ¼ë“¤ : ", sample.columns)
        return sample
    except Exception as e:
        raise ValueError(f"ğŸš¨ ë°ì´í„° ì „ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

def generate_random_id():
    """ëœë¤í•œ 4ìë¦¬ ë¬¸ì + 6ìë¦¬ ìˆ«ìë¡œ ì´ë£¨ì–´ì§„ ID ìƒì„±"""
    letters = ''.join(random.choices(string.ascii_uppercase + string.digits, k=4))  # ëŒ€ë¬¸ì + ìˆ«ì 4ìë¦¬
    numbers = ''.join(random.choices(string.digits, k=6))  # ìˆ«ì 6ìë¦¬
    return f"{letters}{numbers}"


#predict urlë¡œ POST ìš”ì²­ì´ ë“¤ì–´ì˜¤ë©´ predict()ë©”ì„œë“œë¥¼ ìˆ˜í–‰í•˜ê² ë‹¤ëŠ” ì˜ë¯¸
@predict_bp.route("/input/one", methods=["POST"])
def input_one():
    """ ë‹¨ì¼ ì˜ˆì¸¡ ì…ë ¥ â†’ DB ì €ì¥ â†’ ì˜ˆì¸¡ ì‹¤í–‰ """
    user_id, error_response = get_user_id_from_token()
    if error_response:
        return error_response  

    data = {
        "ë§¤ë¬¼í™•ì¸ë°©ì‹": request.form.get("ë§¤ë¬¼í™•ì¸ë°©ì‹"),
        "ì›”ì„¸": float(request.form.get("ì›”ì„¸", 0)),
        "ë³´ì¦ê¸ˆ": float(request.form.get("ë³´ì¦ê¸ˆ", 0)),
        "ê´€ë¦¬ë¹„": float(request.form.get("ê´€ë¦¬ë¹„", 0)),
        "ì „ìš©ë©´ì ": float(request.form.get("ì „ìš©ë©´ì ", 0)),
        "ë°©ìˆ˜": int(request.form.get("ë°©ìˆ˜", 0)),
        "ìš•ì‹¤ìˆ˜": int(request.form.get("ìš•ì‹¤ìˆ˜", 0)),
        "ë°©í–¥": request.form.get("ë°©í–¥"),
        "í•´ë‹¹ì¸µ": int(request.form.get("í•´ë‹¹ì¸µ", 0)),
        "ì´ì¸µ": int(request.form.get("ì´ì¸µ", 0)),
        "ì´ì£¼ì°¨ëŒ€ìˆ˜": int(request.form.get("ì´ì£¼ì°¨ëŒ€ìˆ˜", 0)),
        "ì£¼ì°¨ê°€ëŠ¥ì—¬ë¶€": request.form.get("ì£¼ì°¨ê°€ëŠ¥ì—¬ë¶€"),
        "ì œê³µí”Œë«í¼": request.form.get("ì œê³µí”Œë«í¼"),
        "ì¤‘ê°œì‚¬ë¬´ì†Œ": request.form.get("ì¤‘ê°œì‚¬ë¬´ì†Œ"),
        "ê²Œì¬ì¼": request.form.get("ê²Œì¬ì¼") + " 00:00:00" if request.form.get("ê²Œì¬ì¼") else None
    }

    df = pd.DataFrame([data])
    df['ID'] = generate_random_id()
    df.insert(0, 'ID', df.pop('ID'))
    
    #df.replace([np.inf, -np.inf], np.nan, inplace=True)
    #df.fillna('0', inplace=True)  # NaN ê°’ì„ 0ìœ¼ë¡œ ëŒ€ì²´
    missing_mask = df.isna()  # NaN ìœ„ì¹˜ ì €ì¥
    df = df.astype(object)  # ëª¨ë“  ë°ì´í„°ë¥¼ object íƒ€ì…ìœ¼ë¡œ ë³€í™˜ (ìˆ«ìë„ í¬í•¨)
    df[missing_mask] = "-"  # NaNì´ì—ˆë˜ ê³³ë§Œ "-"ë¡œ ë³€ê²½

    try:
        json_data = json.dumps(df.to_dict(orient="records"), ensure_ascii=False, allow_nan=False)
        new_input = Input(user_id=user_id, input_data=json_data)

        db.session.add(new_input)
        db.session.commit()
        db.session.refresh(new_input)

        return predict_from_db(new_input.id)  # âœ… ë°”ë¡œ ì˜ˆì¸¡ ì‹¤í–‰

    except Exception as e:
        db.session.rollback()
        return jsonify({"error": "ë°ì´í„° ì €ì¥ ì‹¤íŒ¨", "message": str(e)}), 500

@predict_bp.route("/input/file", methods=["POST"])
def input_file():
    """ CSV íŒŒì¼ ì—…ë¡œë“œ â†’ DB ì €ì¥ â†’ ì˜ˆì¸¡ ì‹¤í–‰ """
    user_id, error_response = get_user_id_from_token()
    if error_response:
        return error_response

    file = request.files.get("file")
    if file is None or file.filename == "" or not file.filename.endswith(".csv"):
        return jsonify({"error": "CSV íŒŒì¼ë§Œ ì—…ë¡œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."}), 400

    try:
        df = pd.read_csv(file)
        #df.replace([np.inf, -np.inf], np.nan, inplace=True)
        #df.fillna('0', inplace=True)  # NaN ê°’ì„ 0ìœ¼ë¡œ ëŒ€ì²´
        
        missing_mask = df.isna()  # NaN ìœ„ì¹˜ ì €ì¥
        df = df.astype(object)  # ëª¨ë“  ë°ì´í„°ë¥¼ object íƒ€ì…ìœ¼ë¡œ ë³€í™˜ (ìˆ«ìë„ í¬í•¨)
        df[missing_mask] = "-"  # NaNì´ì—ˆë˜ ê³³ë§Œ "-"ë¡œ ë³€ê²½
        
        json_data = json.dumps(df.to_dict(orient="records"), ensure_ascii=False, allow_nan=False)

        new_input = Input(user_id=user_id if user_id is not None else None, input_data=json_data)

        db.session.add(new_input)
        db.session.commit()
        db.session.refresh(new_input)

        return predict_from_db(new_input.id)  #  ë°”ë¡œ ì˜ˆì¸¡ ì‹¤í–‰

    except Exception as e:
        db.session.rollback()
        return jsonify({"error": "íŒŒì¼ ì €ì¥ ì‹¤íŒ¨", "message": str(e)}), 500

def predict_from_db(input_id):
    """ ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ë¶ˆëŸ¬ì™€ ì˜ˆì¸¡ ìˆ˜í–‰ """
    try:
        input_record = Input.query.get(input_id)
        if not input_record:
            return jsonify({"error": "í•´ë‹¹ IDì˜ ë°ì´í„°ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."}), 404

        df = pd.DataFrame(json.loads(input_record.input_data))

        # "-"ë¥¼ NaNìœ¼ë¡œ ë³€í™˜ (ì›ë˜ ê²°ì¸¡ì¹˜ì˜€ë˜ ê°’)
        df.replace("-", np.nan, inplace=True)
        
        # ë‹¨ì¼ ì…ë ¥, íŒŒì¼ ì…ë ¥ ì „ì²˜ë¦¬ êµ¬ë¶„
        if len(df) == 1:
            preprocessed_df = preprocess_for_one(df)  # ë‹¨ì¼ ì…ë ¥ ì²˜ë¦¬
        else:
            preprocessed_df = preprocess_for_file(df)  #íŒŒì¼ ì…ë ¥ ì²˜ë¦¬

        predictions = model.predict(preprocessed_df)
        pred_proba = model.predict_proba(preprocessed_df)
        correct_probs = pred_proba[np.arange(len(predictions)), predictions]
        confidence_scores = (correct_probs * 100).round(1).astype(float).tolist()

        prediction_labels = ["í—ˆìœ„ë§¤ë¬¼ì´ ì•„ë‹™ë‹ˆë‹¤" if pred == 0 else "í—ˆìœ„ë§¤ë¬¼ì…ë‹ˆë‹¤" for pred in predictions]

        new_prediction = Prediction(
            input_id=input_id,
            prediction_result=prediction_labels,
            confidence=confidence_scores
        )

        db.session.add(new_prediction)
        db.session.commit()

        result_df = df.copy()
        result_df["ì˜ˆì¸¡ ê²°ê³¼"] = prediction_labels[0] if isinstance(prediction_labels, list) else prediction_labels
        result_df["ì‹ ë¢°ë„ (%)"] = confidence_scores[0] if isinstance(confidence_scores, list) else confidence_scores

        result_html = result_df.to_html(classes="table table-striped", index=False)

        return render_template("result.html", table=result_html)

    except Exception as e:
        return jsonify({"error": "ì˜ˆì¸¡ ì‹¤íŒ¨", "message": str(e)}), 400
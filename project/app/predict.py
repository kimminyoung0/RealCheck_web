from flask import Blueprint, request, jsonify, render_template
import jwt
from app import app, db
from app.models import Input, Prediction
import pickle
import pandas as pd
import numpy as np
import xgboost as xgb
import os
import json
import random
import string

predict_bp = Blueprint('predict', __name__)

# ëª¨ë¸ ë¡œë“œ
try:
    BASE_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))  # `app/`ì˜ ìƒìœ„ í´ë”ë¡œ ì´ë™
    MODEL_PATH = os.path.join(BASE_DIR, "model", "final_model.json")  # ëª¨ë¸ ê²½ë¡œ ì„¤ì •

    # ğŸ”¹ ëª¨ë¸ ë¡œë“œ
    model = xgb.XGBClassifier()
    model.load_model(MODEL_PATH) 
except FileNotFoundError:
    raise RuntimeError("âŒ ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

def get_user_id_from_token():
    """ JWT í† í°ì—ì„œ user_id ì¶”ì¶œ """
    token = request.headers.get("Authorization")
    if not token:
        return None, None  # âœ… ë¹„íšŒì›ì´ë©´ None ë°˜í™˜ (ì—ëŸ¬ X)
    try:
        decoded = jwt.decode(token, app.config["SECRET_KEY"], algorithms=["HS256"])
        return decoded["user_id"], None  # âœ… ë¡œê·¸ì¸í•œ íšŒì›ì´ë©´ user_id ë°˜í™˜
    except jwt.ExpiredSignatureError:
        return None, jsonify({"message": "Token expired"})  # âœ… í† í° ë§Œë£Œ
    except jwt.InvalidTokenError:
        return None, jsonify({"message": "Invalid token"})  # âœ… ì˜ëª»ëœ í† í°

def get_season(month):
    """ ì›”(month)ì— ë”°ë¼ ê³„ì ˆ ë°˜í™˜ """
    if month in [3, 4, 5]:
        return 'ë´„'
    elif month in [6, 7, 8]:
        return 'ì—¬ë¦„'
    elif month in [9, 10, 11]:
        return 'ê°€ì„'
    else:
        return 'ê²¨ìš¸'
    
def preprocess_for_file(df):
    try:
        sample = df.copy()
        sample2 = df.copy()

        if sample.empty:
            raise ValueError("âŒ ì…ë ¥ ë°ì´í„°ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")

        # ê²°ì¸¡ ì—¬ë¶€ë¥¼ ì´ì§„ ë³€ìˆ˜ë¡œ ë³€í™˜
        for col in sample2.columns:
            sample2[f'{col}'] = sample2[col].isna().astype(int)
        sample2['ê²°ì¸¡ì¹˜ê°œìˆ˜'] = sample2.sum(axis=1)
        sample['ê²°ì¸¡ì¹˜ê°œìˆ˜'] = sample2['ê²°ì¸¡ì¹˜ê°œìˆ˜']
        
        train_medians = pd.read_csv('./saved/train_medians.csv')
        train_medians_dict = train_medians.set_index("column_name")["median_value"].to_dict()  # train_mediansê°€ DataFrameì´ë¼ë©´ ë³€í™˜
        
        # ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ë§Œ ì¶”ì¶œ
        numeric_cols = [col for col in sample.select_dtypes(include=['number']).columns]
        sample[numeric_cols] = sample[numeric_cols].fillna(sample[numeric_cols].apply(lambda col: train_medians_dict.get(col.name, col.median())))
        
        # ìŠ¤ì¼€ì¼ëŸ¬ ë° KNN ëª¨ë¸ ë¡œë“œ
        scaler = pickle.load(open("./saved/scaler.pkl", "rb"))
        knn_hc = pickle.load(open("./saved/knn_hc.pkl", "rb"))
        knn_dbscan = pickle.load(open("./saved/knn_dbscan.pkl", "rb"))
        X_test = sample[['ì „ìš©ë©´ì ', 'ë°©ìˆ˜', 'ìš•ì‹¤ìˆ˜']]
        X_test_scaled = scaler.transform(X_test)

        print("âœ… X_test_scaled ìƒ˜í”Œ:\n", X_test_scaled[:5])  # ìŠ¤ì¼€ì¼ë§ í›„ ì¼ë¶€ ë°ì´í„° ì¶œë ¥
        print("âœ… X_test_scaled ë°ì´í„° í˜•íƒœ:", X_test_scaled.shape)  # ë³€í™˜ í›„ ì°¨ì› í™•ì¸
        print("âœ… KNN ëª¨ë¸ì´ ê¸°ëŒ€í•˜ëŠ” ì…ë ¥ ì°¨ì›:", knn_hc._fit_X.shape)  # í•™ìŠµëœ KNN ëª¨ë¸ ì°¨ì› í™•ì¸

        # KNN ì˜ˆì¸¡ ìˆ˜í–‰
        sample["ë§¤ë¬¼_HC"] = knn_hc.predict(X_test_scaled)
        print("ğŸ¯ KNN ì˜ˆì¸¡ ì™„ë£Œ! ì²« ë²ˆì§¸ ì˜ˆì¸¡ ê°’:", sample["ë§¤ë¬¼_HC"].iloc[0])

        sample["ë§¤ë¬¼_DBSCAN"] = knn_dbscan.predict(X_test_scaled)
        # ê¸ˆì•¡ ë‹¨ìœ„ ë³€í™˜
        # sample["ë³´ì¦ê¸ˆ"] = sample["ë³´ì¦ê¸ˆ"] / 10000
        # sample["ì›”ì„¸"] = sample["ì›”ì„¸"] / 10000
        
        sample['ì›”ì„¸+ê´€ë¦¬ë¹„'] = sample['ì›”ì„¸'] + sample['ê´€ë¦¬ë¹„']
        sample['ë³´ì¦ê¸ˆ_ì›”ì„¸ê´€ë¦¬ë¹„_ë¹„ìœ¨'] = sample['ì›”ì„¸+ê´€ë¦¬ë¹„'] / sample['ë³´ì¦ê¸ˆ']
        sample['ì „ìš©ë©´ì _ê°€ê²©_ë¹„ìœ¨'] = sample['ë³´ì¦ê¸ˆ_ì›”ì„¸ê´€ë¦¬ë¹„_ë¹„ìœ¨'] / sample['ì „ìš©ë©´ì ']
        
        scaler2 = pickle.load(open("./saved/scaler2.pkl", "rb"))
        knn_kmedoids = pickle.load(open("./saved/knn_kmedoids.pkl", "rb"))
        
        X_test2 = sample[['ë§¤ë¬¼_HC', 'ë§¤ë¬¼_DBSCAN', 'ì „ìš©ë©´ì _ê°€ê²©_ë¹„ìœ¨', 'ë³´ì¦ê¸ˆ_ì›”ì„¸ê´€ë¦¬ë¹„_ë¹„ìœ¨']]
        X_test_scaled2 = scaler2.transform(X_test2)
        
        sample["ì§€ì—­_KMedoids"] = knn_kmedoids.predict(X_test_scaled2)
    
        sample['ê²Œì¬ì¼'] = pd.to_datetime(sample['ê²Œì¬ì¼'], errors='coerce')
        sample['ê³„ì ˆ'] = sample['ê²Œì¬ì¼'].dt.month.apply(get_season)
        
        date_max = pickle.load(open("./saved/date_max.pkl", "rb"))
        sample['ë§¤ë¬¼_ë“±ë¡_ê²½ê³¼ì¼'] = (date_max - sample['ê²Œì¬ì¼']).dt.days
        
        sample = pd.get_dummies(sample, columns=['ë§¤ë¬¼í™•ì¸ë°©ì‹', 'ë°©í–¥', 'ì£¼ì°¨ê°€ëŠ¥ì—¬ë¶€', 'ê³„ì ˆ'], drop_first=True)
        one_hot_columns = [col for col in sample.columns if 'ë§¤ë¬¼í™•ì¸ë°©ì‹' in col or 'ë°©í–¥' in col or 'ì£¼ì°¨ê°€ëŠ¥ì—¬ë¶€' in col or 'ê³„ì ˆ' in col]
        sample[one_hot_columns] = sample[one_hot_columns].astype(int)
        
        sample = sample.drop(columns = ['ID', 'ì¤‘ê°œì‚¬ë¬´ì†Œ', 'ì œê³µí”Œë«í¼', 'ê²Œì¬ì¼', 'ë§¤ë¬¼_DBSCAN', 'ì›”ì„¸+ê´€ë¦¬ë¹„', 'ë³´ì¦ê¸ˆ_ì›”ì„¸ê´€ë¦¬ë¹„_ë¹„ìœ¨'], axis = 1)
        sample = pd.get_dummies(sample, columns=['ë§¤ë¬¼_HC', 'ì§€ì—­_KMedoids'], drop_first=True)
        one_hot_columns = [col for col in sample.columns if 'HC' in col or 'KMedoids' in col]
        sample[one_hot_columns] = sample[one_hot_columns].astype(int)
        return sample
    except Exception as e:
        raise ValueError(f"ğŸš¨ ë°ì´í„° ì „ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
    
def preprocess_for_one(df):
    try:
        sample = df.copy()
        sample2 = df.copy()

        if sample.empty:
            raise ValueError("âŒ ì…ë ¥ ë°ì´í„°ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")

        # ê²°ì¸¡ ì—¬ë¶€ë¥¼ ì´ì§„ ë³€ìˆ˜ë¡œ ë³€í™˜
        for col in sample2.columns:
            sample2[f'{col}'] = sample2[col].isna().astype(int)
        sample2['ê²°ì¸¡ì¹˜ê°œìˆ˜'] = sample2.sum(axis=1)
        sample['ê²°ì¸¡ì¹˜ê°œìˆ˜'] = sample2['ê²°ì¸¡ì¹˜ê°œìˆ˜']
        
        train_medians = pd.read_csv('./saved/train_medians.csv')
        train_medians_dict = train_medians.set_index("column_name")["median_value"].to_dict()  # train_mediansê°€ DataFrameì´ë¼ë©´ ë³€í™˜
        
        # ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ë§Œ ì¶”ì¶œ
        numeric_cols = [col for col in sample.select_dtypes(include=['number']).columns]
        sample[numeric_cols] = sample[numeric_cols].fillna(sample[numeric_cols].apply(lambda col: train_medians_dict.get(col.name, col.median())))
        
        # ìŠ¤ì¼€ì¼ëŸ¬ ë° KNN ëª¨ë¸ ë¡œë“œ
        scaler = pickle.load(open("./saved/scaler.pkl", "rb"))
        knn_hc = pickle.load(open("./saved/knn_hc.pkl", "rb"))
        knn_dbscan = pickle.load(open("./saved/knn_dbscan.pkl", "rb"))
        X_test = sample[['ì „ìš©ë©´ì ', 'ë°©ìˆ˜', 'ìš•ì‹¤ìˆ˜']]
        X_test_scaled = scaler.transform(X_test)

        X_test_scaled = scaler.transform(X_test)  # ìŠ¤ì¼€ì¼ë§ ìˆ˜í–‰

        print("âœ… X_test_scaled ìƒ˜í”Œ:\n", X_test_scaled[:5])  # ìŠ¤ì¼€ì¼ë§ í›„ ì¼ë¶€ ë°ì´í„° ì¶œë ¥
        print("âœ… X_test_scaled ë°ì´í„° í˜•íƒœ:", X_test_scaled.shape)  # ë³€í™˜ í›„ ì°¨ì› í™•ì¸
        print("âœ… KNN ëª¨ë¸ì´ ê¸°ëŒ€í•˜ëŠ” ì…ë ¥ ì°¨ì›:", knn_hc._fit_X.shape)  # í•™ìŠµëœ KNN ëª¨ë¸ ì°¨ì› í™•ì¸

        # KNN ì˜ˆì¸¡ ìˆ˜í–‰
        sample["ë§¤ë¬¼_HC"] = knn_hc.predict(X_test_scaled)
        
        sample["ë§¤ë¬¼_DBSCAN"] = knn_dbscan.predict(X_test_scaled)

        # ê¸ˆì•¡ ë‹¨ìœ„ ë³€í™˜
        # sample["ë³´ì¦ê¸ˆ"] = sample["ë³´ì¦ê¸ˆ"] / 10000
        # sample["ì›”ì„¸"] = sample["ì›”ì„¸"] / 10000
        
        sample['ì›”ì„¸+ê´€ë¦¬ë¹„'] = sample['ì›”ì„¸'] + sample['ê´€ë¦¬ë¹„']
        sample['ë³´ì¦ê¸ˆ_ì›”ì„¸ê´€ë¦¬ë¹„_ë¹„ìœ¨'] = sample['ì›”ì„¸+ê´€ë¦¬ë¹„'] / sample['ë³´ì¦ê¸ˆ']
        sample['ì „ìš©ë©´ì _ê°€ê²©_ë¹„ìœ¨'] = sample['ë³´ì¦ê¸ˆ_ì›”ì„¸ê´€ë¦¬ë¹„_ë¹„ìœ¨'] / sample['ì „ìš©ë©´ì ']
        
        scaler2 = pickle.load(open("./saved/scaler2.pkl", "rb"))
        knn_kmedoids = pickle.load(open("./saved/knn_kmedoids.pkl", "rb"))
        
        X_test2 = sample[['ë§¤ë¬¼_HC', 'ë§¤ë¬¼_DBSCAN', 'ì „ìš©ë©´ì _ê°€ê²©_ë¹„ìœ¨', 'ë³´ì¦ê¸ˆ_ì›”ì„¸ê´€ë¦¬ë¹„_ë¹„ìœ¨']]
        X_test_scaled2 = scaler2.transform(X_test2)
        
        sample["ì§€ì—­_KMedoids"] = knn_kmedoids.predict(X_test_scaled2)
        print("sample.columns:", sample.columns)
        sample['ê²Œì¬ì¼'] = pd.to_datetime(sample['ê²Œì¬ì¼'], errors='coerce')
        sample['ê³„ì ˆ'] = sample['ê²Œì¬ì¼'].dt.month.apply(get_season)
        
        date_max = pickle.load(open("./saved/date_max.pkl", "rb"))
        sample['ë§¤ë¬¼_ë“±ë¡_ê²½ê³¼ì¼'] = (date_max - sample['ê²Œì¬ì¼']).dt.days
        print("ì—¬ê¸°ì„œ ì¤‘ê°„ ì ê²€ sample.ë§¤ë¬¼í™•ì¸ë°©ì‹:", sample["ë§¤ë¬¼í™•ì¸ë°©ì‹"].iloc[0])
        print("ì—¬ê¸°ì„œ ì¤‘ê°„ ì ê²€ sample.ë°©í–¥:", sample["ë°©í–¥"].iloc[0])
        
        # ì›-í•« ì¸ì½”ë”©ì„ ì ìš©í•  ì»¬ëŸ¼ ë° ì œì™¸í•  ê°’
        one_hot_columns = {
            "ë§¤ë¬¼í™•ì¸ë°©ì‹": ["í˜„ì¥í™•ì¸", "ì „í™”í™•ì¸"],  # "ì„œë¥˜í™•ì¸"ì€ ìƒì„±í•˜ì§€ ì•ŠìŒ
            "ë°©í–¥": ["ì„œí–¥", "ë™í–¥", "ë‚¨í–¥", "ë¶ë™í–¥", "ë¶í–¥", "ë‚¨ì„œí–¥", "ë¶ì„œí–¥"],  # "ë‚¨ë™í–¥"ì€ ìƒì„±í•˜ì§€ ì•ŠìŒ
            "ì£¼ì°¨ê°€ëŠ¥ì—¬ë¶€": ["ë¶ˆê°€ëŠ¥"],  # "ê°€ëŠ¥"ì€ ìƒì„±í•˜ì§€ ì•ŠìŒ
            "ê³„ì ˆ": ["ë´„", "ì—¬ë¦„", "ê²¨ìš¸"]  # "ê°€ì„"ì€ ìƒì„±í•˜ì§€ ì•ŠìŒ
        }

        # ì›-í•« ì¸ì½”ë”© ì ìš© (drop_first=True íš¨ê³¼ ì ìš©)
        for col, categories in one_hot_columns.items():
            for cat in categories:
                sample[f"{col}_{cat}"] = (sample[col] == cat).astype(int)

        # ê¸°ì¡´ ì¹´í…Œê³ ë¦¬ ì»¬ëŸ¼ ì‚­ì œ
        sample = sample.drop(columns=one_hot_columns.keys(), errors="ignore")
        
        sample = sample.drop(columns = ['ID', 'ì¤‘ê°œì‚¬ë¬´ì†Œ', 'ì œê³µí”Œë«í¼', 'ê²Œì¬ì¼', 'ë§¤ë¬¼_DBSCAN', 'ì›”ì„¸+ê´€ë¦¬ë¹„', 'ë³´ì¦ê¸ˆ_ì›”ì„¸ê´€ë¦¬ë¹„_ë¹„ìœ¨'], axis = 1)
        print("í•„ìš”ì—†ëŠ” column ì œê±° í›„ ì¤‘ê°„ì ê²€ :", sample.columns)
        
        for i in range(2, 7):  # 2ë¶€í„° 6ê¹Œì§€ ë°˜ë³µ
            sample[f"ë§¤ë¬¼_HC_{i}"] = 0
        
        for i in range(1, 11):  # 1ë¶€í„° 10ê¹Œì§€ ë°˜ë³µ
            sample[f"ì§€ì—­_KMedoids_{i}"] = 0
        
        hc_value = sample['ë§¤ë¬¼_HC'].iloc[0]  # ì²« ë²ˆì§¸ rowì˜ ê°’
        if hc_value != 1:  # ì›í•«ì¸ì½”ë”©ìœ¼ë¡œ ì¸í•´ ë§¤ë¬¼_HC_1 ì»¬ëŸ¼ ìƒì„± x
            sample[f"ë§¤ë¬¼_HC_{hc_value}"] = 1
        
        km_value = sample['ì§€ì—­_KMedoids'].iloc[0]  # ì²« ë²ˆì§¸ rowì˜ ê°’
        if km_value != 0:  # ì›í•«ì¸ì½”ë”©ìœ¼ë¡œ ì¸í•´ ì§€ì—­_KMedoids_0 ì»¬ëŸ¼ ìƒì„± x
            sample[f"ì§€ì—­_KMedoids_{km_value}"] = 1
            
        sample = sample.drop(columns = ['ë§¤ë¬¼_HC', 'ì§€ì—­_KMedoids'], axis = 1)
        print("ì—¬ê¸°ì„œë„ í´ëŸ¬ìŠ¤í„°ë§ ì»¬ëŸ¼ë“¤ ìˆëŠ”ì§€ ì¤‘ê°„ ì ê²€, sample.columns : ", sample.columns)
        print("ìµœì¢… ì»¬ëŸ¼ ìˆ˜ : ", len(sample.columns))
        print("ìµœì¢… ì»¬ëŸ¼ë“¤ : ", sample.columns)
        return sample
    except Exception as e:
        raise ValueError(f"ğŸš¨ ë°ì´í„° ì „ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

def generate_random_id():
    """ëœë¤í•œ 4ìë¦¬ ë¬¸ì + 6ìë¦¬ ìˆ«ìë¡œ ì´ë£¨ì–´ì§„ ID ìƒì„±"""
    letters = ''.join(random.choices(string.ascii_uppercase + string.digits, k=4))  # ëŒ€ë¬¸ì + ìˆ«ì 4ìë¦¬
    numbers = ''.join(random.choices(string.digits, k=6))  # ìˆ«ì 6ìë¦¬
    return f"{letters}{numbers}"


#predict urlë¡œ POST ìš”ì²­ì´ ë“¤ì–´ì˜¤ë©´ predict()ë©”ì„œë“œë¥¼ ìˆ˜í–‰í•˜ê² ë‹¤ëŠ” ì˜ë¯¸
@predict_bp.route("/predict", methods=["POST"])
def predict():
    """ ë‹¨ì¼ ì˜ˆì¸¡ ìˆ˜í–‰ ë° DB ì €ì¥ """
    user_id, error_response = get_user_id_from_token()
    if error_response:
        return error_response  # ì¸ì¦ ì‹¤íŒ¨ ì‹œ ì—ëŸ¬ ë°˜í™˜
    
    print("user_id:", user_id)

    """ FormData ì…ë ¥ì„ ë°›ì•„ì„œ ì˜ˆì¸¡ ìˆ˜í–‰ """
    data = {
        "ë§¤ë¬¼í™•ì¸ë°©ì‹": request.form.get("ë§¤ë¬¼í™•ì¸ë°©ì‹"),
        "ì›”ì„¸": float(request.form.get("ì›”ì„¸", 0)),
        "ë³´ì¦ê¸ˆ": float(request.form.get("ë³´ì¦ê¸ˆ", 0)),
        "ê´€ë¦¬ë¹„": float(request.form.get("ê´€ë¦¬ë¹„", 0)),
        "ì „ìš©ë©´ì ": float(request.form.get("ì „ìš©ë©´ì ", 0)),
        "ë°©ìˆ˜": int(request.form.get("ë°©ìˆ˜", 0)),
        "ìš•ì‹¤ìˆ˜": int(request.form.get("ìš•ì‹¤ìˆ˜", 0)),
        "ë°©í–¥": request.form.get("ë°©í–¥"),
        "í•´ë‹¹ì¸µ": int(request.form.get("í•´ë‹¹ì¸µ", 0)),
        "ì´ì¸µ": int(request.form.get("ì´ì¸µ", 0)),
        "ì´ì£¼ì°¨ëŒ€ìˆ˜": int(request.form.get("ì´ì£¼ì°¨ëŒ€ìˆ˜", 0)),
        "ì£¼ì°¨ê°€ëŠ¥ì—¬ë¶€": request.form.get("ì£¼ì°¨ê°€ëŠ¥ì—¬ë¶€"),
        "ì œê³µí”Œë«í¼": request.form.get("ì œê³µí”Œë«í¼"),
        "ì¤‘ê°œì‚¬ë¬´ì†Œ": request.form.get("ì¤‘ê°œì‚¬ë¬´ì†Œ"),
        "ê²Œì¬ì¼": request.form.get("ê²Œì¬ì¼") + " 00:00:00" if request.form.get("ê²Œì¬ì¼") else None
    }


    try:
        df = pd.DataFrame([data])  # JSON ë°ì´í„°ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜
        print("âœ… ë‹¨ì¼ ì…ë ¥ê°’ ë°ì´í„° í”„ë ˆì„ ìƒì„±")
        df['ID'] = generate_random_id() ############ì¶”í›„ì— user_idê°’ê³¼ ëœë¤ìˆ«ìì˜ì¡°í•©ìœ¼ë¡œ ë§Œë“¤ê¸°
        print(df)
        # ë°ì´í„° ì „ì²˜ë¦¬ ìˆ˜í–‰
        preprocessed_df = preprocess_for_one(df)
        print("âœ… ë‹¨ì¼ ì…ë ¥ê°’ ë°ì´í„° ì „ì²˜ë¦¬ ì™„ë£Œ")
    
        if preprocessed_df.isna().sum().sum() > 0:
            print("ğŸš¨ ì „ì²˜ë¦¬ í›„ì—ë„ NaNì´ ë‚¨ì•„ ìˆìŒ")
            print(preprocessed_df.isna().sum())
            
        preprocessed_df.replace([np.inf, -np.inf], np.nan, inplace=True)  # ë¬´í•œëŒ€ ê°’ì„ NaNìœ¼ë¡œ ë³€í™˜
        preprocessed_df.fillna('-', inplace=True)  # NaNì„ 0ìœ¼ë¡œ ë³€í™˜
        
        preprocessed_df = preprocessed_df[model.feature_names_in_]

        try:
            predictions = model.predict(preprocessed_df)
            print("Predictions:", predictions)  # ì˜ˆì¸¡ ê²°ê³¼ ì¶œë ¥
        except Exception as e:
            print("ì˜ˆì¸¡ ì¤‘ ì˜¤ë¥˜ ë°œìƒ:", e)
        print("ğŸ“Š ë‹¨ì¼ ì…ë ¥ê°’ ëª¨ë¸ ì˜ˆì¸¡ ì™„ë£Œ")

        # ì˜ˆì¸¡ í™•ë¥  ê³„ì‚°
        pred_proba = model.predict_proba(preprocessed_df)
        correct_probs = pred_proba[np.arange(len(predictions)), predictions]
        confidence_scores = (correct_probs * 100).round(1).astype(float).tolist()
        print("confidence_scores : ", confidence_scores)
        # ì˜ˆì¸¡ ê²°ê³¼ ë³€í™˜
        prediction_labels = ["í—ˆìœ„ë§¤ë¬¼ì´ ì•„ë‹™ë‹ˆë‹¤" if pred == 0 else "í—ˆìœ„ë§¤ë¬¼ì…ë‹ˆë‹¤" for pred in predictions]
        print("prediction_labels :", prediction_labels)
        
        # âœ… ë‹¨ì¼ ê°’ë„ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
        if isinstance(confidence_scores, float):  # ë‹¨ì¼ ê°’ì´ë©´ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
            confidence_scores = [confidence_scores]
        if isinstance(prediction_labels, str):  # ë‹¨ì¼ ê°’ì´ë©´ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
            prediction_labels = [prediction_labels]

        # DBì— ì…ë ¥ ë°ì´í„° ì €ì¥í•˜ê¸° ì „ ì „ì²˜ë¦¬
        #df = df.where(pd.notna(df), None)
        df.replace([np.inf, -np.inf], np.nan, inplace=True)  # ë¬´í•œëŒ€ ê°’ì„ NaNìœ¼ë¡œ ë³€í™˜
        df.fillna('-', inplace=True)  # NaNì„ 0ìœ¼ë¡œ ë³€í™˜
        json_data = json.dumps(df.to_dict(orient="records"), allow_nan = False)
        
        # DBì— ì…ë ¥ ë°ì´í„° ì €ì¥
        new_input = Input(user_id=user_id if user_id is not None else None, input_data=json_data)
        #new_input = Input(user_id=user_id, input_data=data)
        
        db.session.add(new_input)
        db.session.commit()

        # DBì— ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥
        new_prediction = Prediction(input_id=new_input.id, 
                                    prediction_result=prediction_labels, 
                                    confidence=confidence_scores)
        db.session.add(new_prediction)
        db.session.commit()

        result_df = df.copy()
        result_df["ì˜ˆì¸¡ ê²°ê³¼"] = prediction_labels[0]
        result_df["ì‹ ë¢°ë„ (%)"] = confidence_scores[0]

        result_html = result_df.to_html(classes="table table-striped", index=False)
        print("predict.pyì˜ predict() ë©”ì„œë“œ ëª¨ë‘ ì™„ë£Œ")

        return render_template("result.html", table=result_html)

    except Exception as e:
        return jsonify({"error": "ì˜ˆì¸¡ ì‹¤íŒ¨", "message": str(e)}), 400


@predict_bp.route("/predict/file", methods=["POST"])
def predict_file():
    """ CSV íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì—¬ ë‹¤ì¤‘ ì˜ˆì¸¡ ìˆ˜í–‰ ë° DB ì €ì¥ """
    
    user_id, error_response = get_user_id_from_token()
    if error_response:
        return error_response
    
    print("user_id:", user_id)
    
    print("ğŸ” ì„œë²„ì—ì„œ ë°›ì€ íŒŒì¼ ëª©ë¡:", request.files)
    file = request.files.get("file")
    print("ğŸ“‚ ì—…ë¡œë“œëœ íŒŒì¼:", file)

    if file is None:
        return jsonify({"error": "íŒŒì¼ì´ ì—…ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."}), 400

    if file.filename == "":
        return jsonify({"error": "íŒŒì¼ì´ ì„ íƒë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."}), 400

    if not file.filename.endswith(".csv"):
        return jsonify({"error": "CSV íŒŒì¼ë§Œ ì—…ë¡œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."}), 400

    try:
        df = pd.read_csv(file)
        print("csv íŒŒì¼ ë°ì´í„° í”„ë ˆì„ ìƒì„±")

        # ë°ì´í„° ì „ì²˜ë¦¬ ìˆ˜í–‰
        preprocessed_df = preprocess_for_file(df)
        print("csv íŒŒì¼ ë°ì´í„° ì „ì²˜ë¦¬ ì™„ë£Œ")
        
        if preprocessed_df.isna().sum().sum() > 0:
            print("ğŸš¨ ì „ì²˜ë¦¬ í›„ì—ë„ NaNì´ ë‚¨ì•„ ìˆìŒ")
            print(preprocessed_df.isna().sum())
        
        preprocessed_df.replace([np.inf, -np.inf], np.nan, inplace=True)  # ë¬´í•œëŒ€ ê°’ì„ NaNìœ¼ë¡œ ë³€í™˜
        preprocessed_df.fillna(0, inplace=True)  # NaNì„ 0ìœ¼ë¡œ ë³€í™˜

        print("preprocessed_df : ", preprocessed_df)
        
        # ì˜ˆì¸¡ ìˆ˜í–‰
        predictions = model.predict(preprocessed_df)
        print("ğŸ“Š csv íŒŒì¼ ë°ì´í„° ëª¨ë¸ ì˜ˆì¸¡ ì™„ë£Œ")
        
        # ì˜ˆì¸¡ í™•ë¥  ê³„ì‚°
        pred_proba = model.predict_proba(preprocessed_df)
        correct_probs = pred_proba[np.arange(len(predictions)), predictions]
        confidence_scores = (correct_probs * 100).round(1).astype(float).tolist()  # âœ… ë¦¬ìŠ¤íŠ¸ ë³€í™˜

        # ì˜ˆì¸¡ ê²°ê³¼ ë³€í™˜
        prediction_labels = ["í—ˆìœ„ë§¤ë¬¼ì´ ì•„ë‹™ë‹ˆë‹¤" if pred == 0 else "í—ˆìœ„ë§¤ë¬¼ì…ë‹ˆë‹¤" for pred in predictions]

        # âœ… JSON ë³€í™˜ì´ í•„ìš”í•œ ê²½ìš°ë§Œ json.dumps() ì‚¬ìš©
        #prediction_labels_json = json.dumps(prediction_labels, ensure_ascii=False)
        #confidence_scores_json = json.dumps(confidence_scores)  # âœ… ë¦¬ìŠ¤íŠ¸ë¥¼ JSON ë¬¸ìì—´ë¡œ ë³€í™˜

        # ì›ë³¸ ë°ì´í„°ë„ JSONìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ì €ì¥
        #df = df.where(pd.notna(df), None)
        df.replace([np.inf, -np.inf], np.nan, inplace=True)
        df.fillna('-', inplace=True)
        json_data = json.dumps(df.to_dict(orient="records"), ensure_ascii=False, allow_nan=False)

        # DBì— ì…ë ¥ ë°ì´í„° ì €ì¥
        new_input = Input(user_id=user_id if user_id is not None else None, input_data=json_data)
        db.session.add(new_input)
        db.session.commit()

        # âœ… DBì— ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥ (ARRAY íƒ€ì…ì„ ì§€ì›í•˜ë©´ ë³€í™˜ ì—†ì´ ì €ì¥)
        new_prediction = Prediction(
            input_id=new_input.id,
            prediction_result=prediction_labels,  # âœ… JSON ì»¬ëŸ¼ì´ë©´ json.dumps() í•„ìš”
            confidence=confidence_scores  # âœ… ë¦¬ìŠ¤íŠ¸ ê·¸ëŒ€ë¡œ ì €ì¥
        )
        db.session.add(new_prediction)
        db.session.commit()

        # ê²°ê³¼ ë°ì´í„°í”„ë ˆì„ ìƒì„±
        result_df = df.copy()
        result_df["ì˜ˆì¸¡ ê²°ê³¼"] = prediction_labels
        result_df["ì‹ ë¢°ë„ (%)"] = confidence_scores

        # HTMLë¡œ ë³€í™˜
        result_html = result_df.to_html(classes="table table-striped", index=False)
        print("âœ… ëª¨ë“  ê³¼ì • ì™„ë£Œ")

        return render_template("result.html", table=result_html)


    except Exception as e:
        return jsonify({"error": "ì˜ˆì¸¡ ì‹¤íŒ¨", "message": str(e)}), 400
from flask import Blueprint, request, jsonify, render_template
import jwt
from app import app, db
from app.models import Input, Prediction
import pickle
import pandas as pd
import numpy as np
import xgboost as xgb
import os

predict_bp = Blueprint('predict', __name__)

# ëª¨ë¸ ë¡œë“œ
try:
    BASE_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))  # `app/`ì˜ ìƒìœ„ í´ë”ë¡œ ì´ë™
    MODEL_PATH = os.path.join(BASE_DIR, "model", "final_model.json")  # ëª¨ë¸ ê²½ë¡œ ì„¤ì •

    # ğŸ”¹ ëª¨ë¸ ë¡œë“œ
    model = xgb.XGBClassifier()
    model.load_model(MODEL_PATH) 
except FileNotFoundError:
    raise RuntimeError("âŒ ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

def get_user_id_from_token():
    """ JWT í† í°ì—ì„œ user_id ì¶”ì¶œ """
    token = request.headers.get("Authorization")
    if not token:
        return None, None  # âœ… ë¹„íšŒì›ì´ë©´ None ë°˜í™˜ (ì—ëŸ¬ X)
    try:
        decoded = jwt.decode(token, app.config["SECRET_KEY"], algorithms=["HS256"])
        return decoded["user_id"], None  # âœ… ë¡œê·¸ì¸í•œ íšŒì›ì´ë©´ user_id ë°˜í™˜
    except jwt.ExpiredSignatureError:
        return None, jsonify({"message": "Token expired"})  # âœ… í† í° ë§Œë£Œ
    except jwt.InvalidTokenError:
        return None, jsonify({"message": "Invalid token"})  # âœ… ì˜ëª»ëœ í† í°

def get_season(month):
    """ ì›”(month)ì— ë”°ë¼ ê³„ì ˆ ë°˜í™˜ """
    if month in [3, 4, 5]:
        return 'ë´„'
    elif month in [6, 7, 8]:
        return 'ì—¬ë¦„'
    elif month in [9, 10, 11]:
        return 'ê°€ì„'
    else:
        return 'ê²¨ìš¸'
    
def preprocess(df):
    try:
        sample = df.copy()
        sample2 = df.copy()

        if sample.empty:
            raise ValueError("âŒ ì…ë ¥ ë°ì´í„°ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")

        # ê²°ì¸¡ ì—¬ë¶€ë¥¼ ì´ì§„ ë³€ìˆ˜ë¡œ ë³€í™˜
        for col in sample2.columns:
            sample2[f'{col}'] = sample2[col].isna().astype(int)
        sample2['ê²°ì¸¡ì¹˜ê°œìˆ˜'] = sample2.sum(axis=1)
        sample['ê²°ì¸¡ì¹˜ê°œìˆ˜'] = sample2['ê²°ì¸¡ì¹˜ê°œìˆ˜']
        
        train_medians = pd.read_csv('./saved/train_medians.csv')
        train_medians_dict = train_medians.set_index("column_name")["median_value"].to_dict()  # train_mediansê°€ DataFrameì´ë¼ë©´ ë³€í™˜
        
        # ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ë§Œ ì¶”ì¶œ
        numeric_cols = [col for col in sample.select_dtypes(include=['number']).columns]
        sample[numeric_cols] = sample[numeric_cols].fillna(sample[numeric_cols].apply(lambda col: train_medians_dict.get(col.name, col.median())))
        
        # ìŠ¤ì¼€ì¼ëŸ¬ ë° KNN ëª¨ë¸ ë¡œë“œ
        scaler = pickle.load(open("./saved/scaler.pkl", "rb"))
        knn_hc = pickle.load(open("./saved/knn_hc.pkl", "rb"))
        knn_dbscan = pickle.load(open("./saved/knn_dbscan.pkl", "rb"))
        X_test = sample[['ì „ìš©ë©´ì ', 'ë°©ìˆ˜', 'ìš•ì‹¤ìˆ˜']]
        X_test_scaled = scaler.transform(X_test)
        
        try:
            print("ğŸ” X_test ë°ì´í„° ìƒ˜í”Œ:\n", X_test.head())  # X_test ì›ë³¸ ë°ì´í„° í™•ì¸
            print("ğŸ” X_test ë°ì´í„° í˜•íƒœ:", X_test.shape)  # X_test ì°¨ì› í™•ì¸
            print("ğŸ” ê²°ì¸¡ê°’ í™•ì¸ (X_test):", X_test.isnull().sum().sum())  # ê²°ì¸¡ê°’ ê°œìˆ˜ í™•ì¸
            print("ğŸ” ë¬´í•œëŒ€ê°’ í™•ì¸ (X_test):", np.isinf(X_test).sum().sum())  # ë¬´í•œëŒ€ê°’ ê°œìˆ˜ í™•ì¸

            print("ğŸ” ìŠ¤ì¼€ì¼ëŸ¬ ê°ì²´ íƒ€ì…:", type(scaler))
            print("ğŸ” ìŠ¤ì¼€ì¼ëŸ¬ ì •ë³´:", scaler)  # ìŠ¤ì¼€ì¼ëŸ¬ê°€ ì˜¬ë°”ë¥´ê²Œ ë¡œë“œë˜ì—ˆëŠ”ì§€ í™•ì¸

            X_test_scaled = scaler.transform(X_test)  # ìŠ¤ì¼€ì¼ë§ ìˆ˜í–‰

            print("âœ… X_test_scaled ìƒ˜í”Œ:\n", X_test_scaled[:5])  # ìŠ¤ì¼€ì¼ë§ í›„ ì¼ë¶€ ë°ì´í„° ì¶œë ¥
            print("âœ… X_test_scaled ë°ì´í„° í˜•íƒœ:", X_test_scaled.shape)  # ë³€í™˜ í›„ ì°¨ì› í™•ì¸
            print("âœ… KNN ëª¨ë¸ì´ ê¸°ëŒ€í•˜ëŠ” ì…ë ¥ ì°¨ì›:", knn_hc._fit_X.shape)  # í•™ìŠµëœ KNN ëª¨ë¸ ì°¨ì› í™•ì¸

            # KNN ì˜ˆì¸¡ ìˆ˜í–‰
            sample["ë§¤ë¬¼_HC"] = knn_hc.predict(X_test_scaled)
            print("ğŸ¯ KNN ì˜ˆì¸¡ ì™„ë£Œ! ì²« ë²ˆì§¸ ì˜ˆì¸¡ ê°’:", sample["ë§¤ë¬¼_HC"].iloc[0])
            
        except ValueError as ve:
            raise ValueError(f"ğŸš¨ KNN ì˜ˆì¸¡ ì¤‘ ì˜¤ë¥˜ ë°œìƒ (ValueError): {str(ve)}")
        except AttributeError as ae:
            raise ValueError(f"ğŸš¨ KNN ì˜ˆì¸¡ ì¤‘ ì˜¤ë¥˜ ë°œìƒ (AttributeError): {str(ae)}")
        except Exception as e:
            raise ValueError(f"ğŸš¨ KNN ì˜ˆì¸¡ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

        sample["ë§¤ë¬¼_DBSCAN"] = knn_dbscan.predict(X_test_scaled)
        
        # ê¸ˆì•¡ ë‹¨ìœ„ ë³€í™˜
        # sample["ë³´ì¦ê¸ˆ"] = sample["ë³´ì¦ê¸ˆ"] / 10000
        # sample["ì›”ì„¸"] = sample["ì›”ì„¸"] / 10000
        
        sample['ì›”ì„¸+ê´€ë¦¬ë¹„'] = sample['ì›”ì„¸'] + sample['ê´€ë¦¬ë¹„']
        sample['ë³´ì¦ê¸ˆ_ì›”ì„¸ê´€ë¦¬ë¹„_ë¹„ìœ¨'] = sample['ì›”ì„¸+ê´€ë¦¬ë¹„'] / sample['ë³´ì¦ê¸ˆ']
        sample['ì „ìš©ë©´ì _ê°€ê²©_ë¹„ìœ¨'] = sample['ë³´ì¦ê¸ˆ_ì›”ì„¸ê´€ë¦¬ë¹„_ë¹„ìœ¨'] / sample['ì „ìš©ë©´ì ']
        
        scaler2 = pickle.load(open("./saved/scaler2.pkl", "rb"))
        knn_kmedoids = pickle.load(open("./saved/knn_kmedoids.pkl", "rb"))
        
        X_test2 = sample[['ë§¤ë¬¼_HC', 'ë§¤ë¬¼_DBSCAN', 'ì „ìš©ë©´ì _ê°€ê²©_ë¹„ìœ¨', 'ë³´ì¦ê¸ˆ_ì›”ì„¸ê´€ë¦¬ë¹„_ë¹„ìœ¨']]
        X_test_scaled2 = scaler2.transform(X_test2)
        
        sample["ì§€ì—­_KMedoids"] = knn_kmedoids.predict(X_test_scaled2)
        sample['ê²Œì¬ì¼'] = pd.to_datetime(sample['ê²Œì¬ì¼'], errors='coerce')
        sample['ê³„ì ˆ'] = sample['ê²Œì¬ì¼'].dt.month.apply(get_season)

        date_max = pickle.load(open("./saved/date_max.pkl", "rb"))
        sample['ë§¤ë¬¼_ë“±ë¡_ê²½ê³¼ì¼'] = (date_max - sample['ê²Œì¬ì¼']).dt.days

        sample = pd.get_dummies(sample, columns=['ë§¤ë¬¼í™•ì¸ë°©ì‹', 'ë°©í–¥', 'ì£¼ì°¨ê°€ëŠ¥ì—¬ë¶€', 'ê³„ì ˆ'], drop_first=True)
        sample = sample.drop(columns = ['ID', 'ì¤‘ê°œì‚¬ë¬´ì†Œ', 'ì œê³µí”Œë«í¼', 'ê²Œì¬ì¼', 'ë§¤ë¬¼_DBSCAN', 'ì›”ì„¸+ê´€ë¦¬ë¹„', 'ë³´ì¦ê¸ˆ_ì›”ì„¸ê´€ë¦¬ë¹„_ë¹„ìœ¨'], axis = 1)
        sample = pd.get_dummies(sample, columns=['ë§¤ë¬¼_HC', 'ì§€ì—­_KMedoids'], drop_first=True)
        one_hot_columns = [col for col in sample.columns if 'HC' in col or 'KMedoids' in col]
        sample[one_hot_columns] = sample[one_hot_columns].astype(int)
        return sample
    except Exception as e:
        raise ValueError(f"ğŸš¨ ë°ì´í„° ì „ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

def make_prediction(input_data):
    try:
        df = pd.DataFrame([input_data])
        preprocessed_df = preprocess(df)
        prediction = model.predict(preprocessed_df)
        pred_proba = model.predict_proba(preprocessed_df)
        correct_probs = pred_proba[np.arange(len(prediction)), prediction]
        percent_probs_mean = (correct_probs * 100).round(1).mean()
        return prediction, percent_probs_mean
    except Exception as e:
        raise ValueError(f"ğŸš¨ ì˜ˆì¸¡ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

#predict urlë¡œ POST ìš”ì²­ì´ ë“¤ì–´ì˜¤ë©´ predict()ë©”ì„œë“œë¥¼ ìˆ˜í–‰í•˜ê² ë‹¤ëŠ” ì˜ë¯¸
@predict_bp.route("/predict", methods=["POST"])
def predict():
    """ JSON ì…ë ¥ì„ ë°›ì•„ì„œ ë‹¨ì¼ ì˜ˆì¸¡ ìˆ˜í–‰ """
    user_id, error_response = get_user_id_from_token()
    if error_response:
        return error_response  # ì¸ì¦ ì‹¤íŒ¨ ì‹œ ì—ëŸ¬ ë°˜í™˜
    
    data = request.json

    try:
        prediction_result, percent_probs_mean = make_prediction(data)
    except ValueError as e:
        return jsonify({"error": str(e)}), 400

    new_input = Input(user_id=user_id, input_data=data)
    db.session.add(new_input)
    db.session.commit()

    new_prediction = Prediction(input_id=new_input.id, 
                                prediction_result=prediction_result, 
                                percent_probs_mean=percent_probs_mean)
    db.session.add(new_prediction)
    db.session.commit()

    return jsonify({"prediction": prediction_result, "pred_proba": percent_probs_mean})

@predict_bp.route("/predict/file", methods=["POST"])
def predict_file():
    """ CSV íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì—¬ ë‹¤ì¤‘ ì˜ˆì¸¡ ìˆ˜í–‰ """
    user_id, error_response = get_user_id_from_token()
    if error_response:
        return error_response
    print("ğŸ” ì„œë²„ì—ì„œ ë°›ì€ íŒŒì¼ ëª©ë¡:", request.files)
    file = request.files.get("file")
    print("ğŸ“‚ ì—…ë¡œë“œëœ íŒŒì¼:", file)
    ####################ì´ ìœ„ê¹Œì§€ë§Œ ì‹¤í–‰ë¨
    if file is None:
        return jsonify({"error": "íŒŒì¼ì´ ì—…ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."}), 400

    if file.filename == "":
        return jsonify({"error": "íŒŒì¼ì´ ì„ íƒë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."}), 400

    if not file.filename.endswith(".csv"):
        return jsonify({"error": "CSV íŒŒì¼ë§Œ ì—…ë¡œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."}), 400
    print("ì—¬ê¸°ê¹Œì§€ ìˆ˜í–‰ë¨")
    try:
        df = pd.read_csv(file)
        print("df ë°ì´í„° í”„ë ˆì„ ìƒì„±!!!!!!!!!")
        # ë°ì´í„° ì „ì²˜ë¦¬ ìˆ˜í–‰
        preprocessed_df = preprocess(df)
        print("df ë°ì´í„° ì „ì²˜ë¦¬ ì™„ë£Œ !!!!!!!!!")
        predictions = model.predict(preprocessed_df)
        print("df ëª¨ë¸ ì˜ˆì¸¡ ì™„ë£Œ !!!!!!!!!")
        pred_proba = model.predict_proba(preprocessed_df)
        correct_probs = pred_proba[np.arange(len(predictions)), predictions]
        confidence_scores = (correct_probs * 100).round(1)

        prediction_labels = ["í—ˆìœ„ë§¤ë¬¼ì´ ì•„ë‹™ë‹ˆë‹¤" if pred == 0 else "í—ˆìœ„ë§¤ë¬¼ì…ë‹ˆë‹¤" for pred in predictions]

        result_df = df.copy()
        result_df["ì˜ˆì¸¡ ê²°ê³¼"] = prediction_labels
        result_df["ì‹ ë¢°ë„ (%)"] = confidence_scores

        result_html = result_df.to_html(classes="table table-striped", index=False)

        return render_template("result.html", table=result_html)

    except Exception as e:
        return jsonify({"error": "íŒŒì¼ ì˜ˆì¸¡ ì‹¤íŒ¨", "message": str(e)}), 400

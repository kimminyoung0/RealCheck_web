from flask import Blueprint, request, jsonify, render_template
import jwt
from app import app, db
from app.models import Input, Prediction
import pickle
import pandas as pd
import numpy as np
import xgboost as xgb
import os
import json
import random
import string

predict_bp = Blueprint('predict', __name__)

# ëª¨ë¸ ë¡œë“œ
try:
    BASE_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))  # `app/`ì˜ ìƒìœ„ í´ë”ë¡œ ì´ë™
    MODEL_PATH = os.path.join(BASE_DIR, "model", "final_model.json")  # ëª¨ë¸ ê²½ë¡œ ì„¤ì •

    # ğŸ”¹ ëª¨ë¸ ë¡œë“œ
    model = xgb.XGBClassifier()
    model.load_model(MODEL_PATH) 
except FileNotFoundError:
    raise RuntimeError("âŒ ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

def get_user_id_from_token():
    """ JWT í† í°ì—ì„œ user_id ì¶”ì¶œ """
    token = request.headers.get("Authorization")
    if not token:
        return None, None  # âœ… ë¹„íšŒì›ì´ë©´ None ë°˜í™˜ (ì—ëŸ¬ X)
    try:
        decoded = jwt.decode(token, app.config["SECRET_KEY"], algorithms=["HS256"])
        return decoded["user_id"], None  # âœ… ë¡œê·¸ì¸í•œ íšŒì›ì´ë©´ user_id ë°˜í™˜
    except jwt.ExpiredSignatureError:
        return None, jsonify({"message": "Token expired"})  # âœ… í† í° ë§Œë£Œ
    except jwt.InvalidTokenError:
        return None, jsonify({"message": "Invalid token"})  # âœ… ì˜ëª»ëœ í† í°

def get_season(month):
    """ ì›”(month)ì— ë”°ë¼ ê³„ì ˆ ë°˜í™˜ """
    if month in [3, 4, 5]:
        return 'ë´„'
    elif month in [6, 7, 8]:
        return 'ì—¬ë¦„'
    elif month in [9, 10, 11]:
        return 'ê°€ì„'
    else:
        return 'ê²¨ìš¸'
    
def preprocess(df):
    try:
        sample = df.copy()
        sample2 = df.copy()

        if sample.empty:
            raise ValueError("âŒ ì…ë ¥ ë°ì´í„°ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")

        # ê²°ì¸¡ ì—¬ë¶€ë¥¼ ì´ì§„ ë³€ìˆ˜ë¡œ ë³€í™˜
        for col in sample2.columns:
            sample2[f'{col}'] = sample2[col].isna().astype(int)
        sample2['ê²°ì¸¡ì¹˜ê°œìˆ˜'] = sample2.sum(axis=1)
        sample['ê²°ì¸¡ì¹˜ê°œìˆ˜'] = sample2['ê²°ì¸¡ì¹˜ê°œìˆ˜']
        
        train_medians = pd.read_csv('./saved/train_medians.csv')
        train_medians_dict = train_medians.set_index("column_name")["median_value"].to_dict()  # train_mediansê°€ DataFrameì´ë¼ë©´ ë³€í™˜
        
        # ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ë§Œ ì¶”ì¶œ
        numeric_cols = [col for col in sample.select_dtypes(include=['number']).columns]
        sample[numeric_cols] = sample[numeric_cols].fillna(sample[numeric_cols].apply(lambda col: train_medians_dict.get(col.name, col.median())))
        
        # ìŠ¤ì¼€ì¼ëŸ¬ ë° KNN ëª¨ë¸ ë¡œë“œ
        scaler = pickle.load(open("./saved/scaler.pkl", "rb"))
        knn_hc = pickle.load(open("./saved/knn_hc.pkl", "rb"))
        knn_dbscan = pickle.load(open("./saved/knn_dbscan.pkl", "rb"))
        
        X_test = sample[['ì „ìš©ë©´ì ', 'ë°©ìˆ˜', 'ìš•ì‹¤ìˆ˜']]
        X_test_scaled = scaler.transform(X_test)
        #X_test_scaled = scaler.transform(X_test.values.reshape(1, -1) if len(X_test.shape) == 1 else X_test)
        print(f"âœ… X_test_scaled shape: {X_test_scaled.shape}")
        
        sample["ë§¤ë¬¼_HC"] = knn_hc.predict(X_test_scaled)

        sample["ë§¤ë¬¼_DBSCAN"] = knn_dbscan.predict(X_test_scaled)
        
        print(f"âœ… ë§¤ë¬¼_DBSCAN shape: {sample['ë§¤ë¬¼_DBSCAN'].shape}")
        
        # ê¸ˆì•¡ ë‹¨ìœ„ ë³€í™˜
        # sample["ë³´ì¦ê¸ˆ"] = sample["ë³´ì¦ê¸ˆ"] / 10000
        # sample["ì›”ì„¸"] = sample["ì›”ì„¸"] / 10000
        
        sample['ì›”ì„¸+ê´€ë¦¬ë¹„'] = sample['ì›”ì„¸'] + sample['ê´€ë¦¬ë¹„']
        sample['ë³´ì¦ê¸ˆ_ì›”ì„¸ê´€ë¦¬ë¹„_ë¹„ìœ¨'] = sample['ì›”ì„¸+ê´€ë¦¬ë¹„'] / sample['ë³´ì¦ê¸ˆ']
        sample['ì „ìš©ë©´ì _ê°€ê²©_ë¹„ìœ¨'] = sample['ë³´ì¦ê¸ˆ_ì›”ì„¸ê´€ë¦¬ë¹„_ë¹„ìœ¨'] / sample['ì „ìš©ë©´ì ']
        print("1111111")
        
        scaler2 = pickle.load(open("./saved/scaler2.pkl", "rb"))
        knn_kmedoids = pickle.load(open("./saved/knn_kmedoids.pkl", "rb"))
        print("2222222")
        
        X_test2 = sample[['ë§¤ë¬¼_HC', 'ë§¤ë¬¼_DBSCAN', 'ì „ìš©ë©´ì _ê°€ê²©_ë¹„ìœ¨', 'ë³´ì¦ê¸ˆ_ì›”ì„¸ê´€ë¦¬ë¹„_ë¹„ìœ¨']]
        #X_test_scaled2 = scaler2.transform(X_test2)
        X_test_scaled2= scaler.transform(X_test2.values.reshape(1, -1) if len(X_test2.shape) == 1 else X_test2)

        print(f"âœ… X_test_scaled2 shape: {X_test_scaled2.shape}")
        
        #X_test_scaled2 = scaler2.transform(np.atleast_2d(X_test2))
        #X_test_scaled2 = pd.DataFrame(X_test_scaled2, columns=X_test2.columns, index=X_test2.index)
        print("3333333")
        sample["ì§€ì—­_KMedoids"] = knn_kmedoids.predict(X_test_scaled2)
        #sample["ì§€ì—­_KMedoids"] = np.array(knn_kmedoids.predict(X_test_scaled2)).reshape(-1)
        print(f"âœ… ì§€ì—­_KMedoids shape: {sample['ì§€ì—­_KMedoids'].shape}")
        print("4444444")
        sample['ê²Œì¬ì¼'] = pd.to_datetime(sample['ê²Œì¬ì¼'], errors='coerce')
        sample['ê³„ì ˆ'] = sample['ê²Œì¬ì¼'].dt.month.apply(get_season)
        print("55555")
        date_max = pickle.load(open("./saved/date_max.pkl", "rb"))
        sample['ë§¤ë¬¼_ë“±ë¡_ê²½ê³¼ì¼'] = (date_max - sample['ê²Œì¬ì¼']).dt.days
        print("666666")
        sample = pd.get_dummies(sample, columns=['ë§¤ë¬¼í™•ì¸ë°©ì‹', 'ë°©í–¥', 'ì£¼ì°¨ê°€ëŠ¥ì—¬ë¶€', 'ê³„ì ˆ'], drop_first=True)
        sample = sample.drop(columns = ['ID', 'ì¤‘ê°œì‚¬ë¬´ì†Œ', 'ì œê³µí”Œë«í¼', 'ê²Œì¬ì¼', 'ë§¤ë¬¼_DBSCAN', 'ì›”ì„¸+ê´€ë¦¬ë¹„', 'ë³´ì¦ê¸ˆ_ì›”ì„¸ê´€ë¦¬ë¹„_ë¹„ìœ¨'], axis = 1)
        sample = pd.get_dummies(sample, columns=['ë§¤ë¬¼_HC', 'ì§€ì—­_KMedoids'], drop_first=True)
        one_hot_columns = [col for col in sample.columns if 'HC' in col or 'KMedoids' in col]
        sample[one_hot_columns] = sample[one_hot_columns].astype(int)
        print("sample data: ", sample)
        return sample
    except Exception as e:
        raise ValueError(f"ğŸš¨ ë°ì´í„° ì „ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

def generate_random_id():
    """ëœë¤í•œ 4ìë¦¬ ë¬¸ì + 6ìë¦¬ ìˆ«ìë¡œ ì´ë£¨ì–´ì§„ ID ìƒì„±"""
    letters = ''.join(random.choices(string.ascii_uppercase + string.digits, k=4))  # ëŒ€ë¬¸ì + ìˆ«ì 4ìë¦¬
    numbers = ''.join(random.choices(string.digits, k=6))  # ìˆ«ì 6ìë¦¬
    return f"{letters}{numbers}"


def make_prediction(input_data):
    try:
        df = pd.DataFrame([input_data])
        preprocessed_df = preprocess(df)
        prediction = model.predict(preprocessed_df)
        pred_proba = model.predict_proba(preprocessed_df)
        correct_probs = pred_proba[np.arange(len(prediction)), prediction]
        percent_probs_mean = (correct_probs * 100).round(1).mean()
        return prediction, percent_probs_mean
    except Exception as e:
        raise ValueError(f"ğŸš¨ ì˜ˆì¸¡ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

#predict urlë¡œ POST ìš”ì²­ì´ ë“¤ì–´ì˜¤ë©´ predict()ë©”ì„œë“œë¥¼ ìˆ˜í–‰í•˜ê² ë‹¤ëŠ” ì˜ë¯¸
@predict_bp.route("/predict", methods=["POST"])
def predict():
    """ JSON ì…ë ¥ì„ ë°›ì•„ ë‹¨ì¼ ì˜ˆì¸¡ ìˆ˜í–‰ ë° DB ì €ì¥ """
    user_id, error_response = get_user_id_from_token()
    if error_response:
        return error_response  # ì¸ì¦ ì‹¤íŒ¨ ì‹œ ì—ëŸ¬ ë°˜í™˜

    data = request.json
    print("ğŸ”ì…ë ¥ ë°›ì€ data", data)
    if not data:
        return jsonify({"error": "ì…ë ¥ ë°ì´í„°ê°€ ì œê³µë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."}), 400

    try:
        df = pd.DataFrame([data])  # JSON ë°ì´í„°ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜
        print("df ë°ì´í„° í”„ë ˆì„ ìƒì„±!!!!!!!!!2")
        df['ID'] = generate_random_id() ############ì¶”í›„ì— user_idê°’ê³¼ ëœë¤ìˆ«ìì˜ì¡°í•©ìœ¼ë¡œ ë§Œë“¤ê¸°
        print(df)
        # ë°ì´í„° ì „ì²˜ë¦¬ ìˆ˜í–‰
        preprocessed_df = preprocess(df)
        print("df ë°ì´í„° ì „ì²˜ë¦¬ ì™„ë£Œ !!!!!!!!!")
    
        if preprocessed_df.isna().sum().sum() > 0:
            print("ğŸš¨ ì „ì²˜ë¦¬ í›„ì—ë„ NaNì´ ë‚¨ì•„ ìˆìŒ")
            print(preprocessed_df.isna().sum())
            
        preprocessed_df.replace([np.inf, -np.inf], np.nan, inplace=True)  # ë¬´í•œëŒ€ ê°’ì„ NaNìœ¼ë¡œ ë³€í™˜
        preprocessed_df.fillna('-', inplace=True)  # NaNì„ 0ìœ¼ë¡œ ë³€í™˜
        print("ë³€í™˜ì™„ë£Œ!!!!!!!!!!!")
        print(preprocessed_df)
        # ì˜ˆì¸¡ ìˆ˜í–‰####################ì•„ë˜ì—ì„œë¶€í„°ë¬¸ì œë°œìƒì—¬ê¸°ì„œë¶€í„°í•´ê²°í•˜ê¸°
        predictions = model.predict(preprocessed_df)
        print("df ëª¨ë¸ ì˜ˆì¸¡ ì™„ë£Œ !!!!!!!!!")

        print("ì´ í•¨ìˆ˜ê°€ ì‹¤í–‰ë˜ëŠ”ê±° ë§ã…ˆë‹ˆ........")
        try:
            print("tryë¬¸ ë“¤ì–´ì˜´.......")
            # ì˜ˆì¸¡ í™•ë¥  ê³„ì‚°
            pred_proba = model.predict_proba(preprocessed_df)
            print("pred_proba : ", pred_proba)
            # ì°¨ì› ë¬¸ì œ í•´ê²°
            predictions = np.array(predictions).flatten()  # 1ì°¨ì› ë°°ì—´ë¡œ ë³€í™˜
            print("predictions : ", predictions)
            correct_probs = pred_proba[np.arange(len(predictions)), predictions]  # ì•ˆì „í•œ ì¸ë±ì‹±
            print("correct_probs : ", correct_probs)
            confidence_scores = (correct_probs * 100).round(1).astype(float)
            print("confidence_scores : ", confidence_scores)
            # ì˜ˆì¸¡ ê²°ê³¼ ë³€í™˜
            prediction_labels = ["í—ˆìœ„ë§¤ë¬¼ì´ ì•„ë‹™ë‹ˆë‹¤" if pred == 0 else "í—ˆìœ„ë§¤ë¬¼ì…ë‹ˆë‹¤" for pred in predictions]
        except Exception as e:
            print("ğŸš¨ ì˜ˆì¸¡ ê³¼ì •ì—ì„œ ì˜¤ë¥˜ ë°œìƒ:", str(e))
            import traceback
            traceback.print_exc()  # ìƒì„¸í•œ ì˜¤ë¥˜ ë¡œê·¸ ì¶œë ¥
            return "ì˜ˆì¸¡ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: " + str(e), 400 
        
        try:
            # DBì— ì…ë ¥ ë°ì´í„° ì €ì¥
            df = df.where(pd.notna(df), None)
            df.replace([np.inf, -np.inf], np.nan, inplace=True)  # ë¬´í•œëŒ€ ê°’ì„ NaNìœ¼ë¡œ ë³€í™˜
            df.fillna('-', inplace=True)  # NaNì„ 0ìœ¼ë¡œ ë³€í™˜

            json_data = json.dumps(df.to_dict(orient="records"), allow_nan = False)
            new_input = Input(user_id=user_id if user_id is not None else None, input_data=json_data)

            db.session.add(new_input)
            db.session.commit()

            # DBì— ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥
            for pred, conf in zip(prediction_labels, confidence_scores):
                new_prediction = Prediction(input_id=new_input.id, 
                                            prediction_result=pred, 
                                            confidence=conf)
                db.session.add(new_prediction)

            db.session.commit()
        except Exception as e:
            print("ğŸš¨ DB ì €ì¥ ê³¼ì •ì—ì„œ ì˜¤ë¥˜ ë°œìƒ:", str(e))
            import traceback
            traceback.print_exc()  # ìƒì„¸í•œ ì˜¤ë¥˜ ë¡œê·¸ ì¶œë ¥
            return "DB ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: " + str(e), 400 
        

        result_df = df.copy()
        result_df["ì˜ˆì¸¡ ê²°ê³¼"] = prediction_labels
        result_df["ì‹ ë¢°ë„ (%)"] = confidence_scores

        result_html = result_df.to_html(classes="table table-striped", index=False)
        print("predict.pyì˜ predict() ë©”ì„œë“œ ëª¨ë‘ ì™„ë£Œ")

        return render_template("result.html", table=result_html)

    except Exception as e:
        return jsonify({"error": "ì˜ˆì¸¡ ì‹¤íŒ¨", "message": str(e)}), 400


@predict_bp.route("/predict/file", methods=["POST"])
def predict_file():
    """ CSV íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì—¬ ë‹¤ì¤‘ ì˜ˆì¸¡ ìˆ˜í–‰ ë° DB ì €ì¥ """
    user_id, error_response = get_user_id_from_token()
    if error_response:
        return error_response
    print("ğŸ” ì„œë²„ì—ì„œ ë°›ì€ íŒŒì¼ ëª©ë¡:", request.files)
    file = request.files.get("file")
    print("ğŸ“‚ ì—…ë¡œë“œëœ íŒŒì¼:", file)

    if file is None:
        return jsonify({"error": "íŒŒì¼ì´ ì—…ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."}), 400

    if file.filename == "":
        return jsonify({"error": "íŒŒì¼ì´ ì„ íƒë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."}), 400

    if not file.filename.endswith(".csv"):
        return jsonify({"error": "CSV íŒŒì¼ë§Œ ì—…ë¡œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."}), 400

    try:
        df = pd.read_csv(file)
        print("df ë°ì´í„° í”„ë ˆì„ ìƒì„±1")

        # ë°ì´í„° ì „ì²˜ë¦¬ ìˆ˜í–‰
        preprocessed_df = preprocess(df)
        print("df ë°ì´í„° ì „ì²˜ë¦¬ ì™„ë£Œ !!!!!!!!!")
        
        if preprocessed_df.isna().sum().sum() > 0:
            print("ğŸš¨ ì „ì²˜ë¦¬ í›„ì—ë„ NaNì´ ë‚¨ì•„ ìˆìŒ")
            print(preprocessed_df.isna().sum())
        
        preprocessed_df.replace([np.inf, -np.inf], np.nan, inplace=True)  # ë¬´í•œëŒ€ ê°’ì„ NaNìœ¼ë¡œ ë³€í™˜
        preprocessed_df.fillna(0, inplace=True)  # NaNì„ 0ìœ¼ë¡œ ë³€í™˜

        print(preprocessed_df)
        # ì˜ˆì¸¡ ìˆ˜í–‰
        predictions = model.predict(preprocessed_df)
        print("df ëª¨ë¸ ì˜ˆì¸¡ ì™„ë£Œ !!!!!!!!!")

        # ì˜ˆì¸¡ í™•ë¥  ê³„ì‚°
        pred_proba = model.predict_proba(preprocessed_df)
        correct_probs = pred_proba[np.arange(len(predictions)), predictions]
        confidence_scores = (correct_probs * 100).round(1).astype(float)

        # ì˜ˆì¸¡ ê²°ê³¼ ë³€í™˜
        prediction_labels = ["í—ˆìœ„ë§¤ë¬¼ì´ ì•„ë‹™ë‹ˆë‹¤" if pred == 0 else "í—ˆìœ„ë§¤ë¬¼ì…ë‹ˆë‹¤" for pred in predictions]

        # DBì— ì…ë ¥ ë°ì´í„° ì €ì¥
        df = df.where(pd.notna(df), None)
        df.replace([np.inf, -np.inf], np.nan, inplace=True)  # ë¬´í•œëŒ€ ê°’ì„ NaNìœ¼ë¡œ ë³€í™˜
        df.fillna('-', inplace=True)  # NaNì„ 0ìœ¼ë¡œ ë³€í™˜

        json_data = json.dumps(df.to_dict(orient="records"), allow_nan = False)
        new_input = Input(user_id=user_id if user_id is not None else None, input_data=json_data)

        db.session.add(new_input)
        db.session.commit()

        # DBì— ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥
        for pred, conf in zip(prediction_labels, confidence_scores):
            new_prediction = Prediction(input_id=new_input.id, 
                                        prediction_result=pred, 
                                        confidence=conf)
            db.session.add(new_prediction)

        db.session.commit()

        result_df = df.copy()
        result_df["ì˜ˆì¸¡ ê²°ê³¼"] = prediction_labels
        result_df["ì‹ ë¢°ë„ (%)"] = confidence_scores

        result_html = result_df.to_html(classes="table table-striped", index=False)
        print("predict.pyì˜ predict_file() ë©”ì„œë“œ ëª¨ë‘ ì™„ë£Œ")

        return render_template("result.html", table=result_html)

    except Exception as e:
        return jsonify({"error": "íŒŒì¼ ì˜ˆì¸¡ ì‹¤íŒ¨", "message": str(e)}), 400
